## Прямое распространение (Forward Propagation)

```В данном описании явно прописано о скалярной форме записи формулы. Во многих источниках формулы заданы в векторной форме, поэтому важно чтобы складывалось понимание общих формул в ЛЮБОМ их виде.```

Прямое распространение — это процесс вычисления выходных значений нейронной сети на основе входных данных путем последовательного прохождения сигнала через все слои сети от входа к выходу.

## Алгоритм прямого распространения по шагам

### (1) Формула вычисления взвешенной суммы (NET входа нейрона)

**Скалярная форма**

$$ s_j^q = \sum_i w_{ij}^q \cdot y_i^{q-1} + b_j^q $$

*где $s_j^q$ — взвешенная сумма (NET вход) $j$-го нейрона слоя $q$; $w_{ij}^q$ — вес связи от $i$-го нейрона слоя $q-1$ к $j$-му нейрону слоя $q$; $y_i^{q-1}$ — выход $i$-го нейрона предыдущего слоя $q-1$; $b_j^q$ — смещение (bias) $j$-го нейрона слоя $q$; суммирование ведется по всем нейронам предыдущего слоя.*


**Векторная форма**

$$ z^q = W^q \cdot y^{q-1} + b^q $$

*где $z^q$ — вектор взвешенных сумм слоя $q$; $W^q$ — матрица весов слоя $q$; $y^{q-1}$ — вектор выходов предыдущего слоя $q-1$; $b^q$ — вектор смещений слоя $q$.*


### (2) Формула вычисления выхода нейрона (применение функции активации)

**Скалярная форма**

$$ y_j^q = f(s_j^q) $$

*где $y_j^q$ — выход $j$-го нейрона слоя $q$; $f$ — функция активации (сигмоид, ReLU, tanh и т.д.); $s_j^q$ — взвешенная сумма $j$-го нейрона слоя $q$.*


**Векторная форма**

$$ y^q = f(z^q) $$

*где $y^q$ — вектор выходов слоя $q$; $f$ — функция активации, применяемая поэлементно; $z^q$ — вектор взвешенных сумм слоя $q$.*


### (3) Формула для входного слоя

**Скалярная форма**

$$ y_i^0 = x_i $$

*где $y_i^0$ — выход $i$-го нейрона входного слоя (слой 0); $x_i$ — $i$-е входное значение.*


**Векторная форма**

$$ y^0 = x $$

*где $y^0$ — вектор выходов входного слоя; $x$ — вектор входных значений.*


### (4) Общая формула прямого распространения через все слои

**Скалярная форма (для слоя q)**

$$ y_j^q = f\left(\sum_i w_{ij}^q \cdot y_i^{q-1} + b_j^q\right) $$

*где вычисление производится последовательно для каждого слоя $q = 1, 2, ..., Q$, где $Q$ — выходной слой.*


**Векторная форма (для всей сети)**

$$ 
\begin{align*}
y^1 &= f(W^1 \cdot x + b^1) \\
y^2 &= f(W^2 \cdot y^1 + b^2) \\
&\vdots \\
y^Q &= f(W^Q \cdot y^{Q-1} + b^Q)
\end{align*}
$$

*где последовательно вычисляются выходы каждого слоя, начиная с первого скрытого слоя и заканчивая выходным слоем $Q$.*


## Пошаговый алгоритм прямого распространения

**ШАГ 1:** Инициализировать входной слой значениями входного вектора $x$.

**ШАГ 2:** Для первого скрытого слоя ($q = 1$):
- Вычислить взвешенную сумму для каждого нейрона по формуле (1): $s_j^1 = \sum_i w_{ij}^1 \cdot x_i + b_j^1$
- Применить функцию активации по формуле (2): $y_j^1 = f(s_j^1)$

**ШАГ 3:** Для каждого последующего скрытого слоя ($q = 2, 3, ..., Q-1$):
- Вычислить взвешенную сумму для каждого нейрона: $s_j^q = \sum_i w_{ij}^q \cdot y_i^{q-1} + b_j^q$
- Применить функцию активации: $y_j^q = f(s_j^q)$

**ШАГ 4:** Для выходного слоя ($q = Q$):
- Вычислить взвешенную сумму для каждого выходного нейрона: $s_j^Q = \sum_i w_{ij}^Q \cdot y_i^{Q-1} + b_j^Q$
- Применить функцию активации (может отличаться от функции активации скрытых слоев): $y_j^Q = f(s_j^Q)$

**ШАГ 5:** Выходной вектор $y^Q$ является результатом прямого распространения и представляет собой предсказание нейронной сети для данного входного вектора $x$.

**ШАГ 6:** Сохранить промежуточные значения $s_j^q$ и $y_j^q$ для всех слоев — они потребуются на этапе обратного распространения ошибки для вычисления градиентов и обновления весов.


## Важные замечания

- Прямое распространение — это детерминированный процесс: для фиксированных весов и одного и того же входа всегда получается один и тот же выход.
- Выбор функции активации $f$ существенно влияет на способность сети к обучению и её выразительность.
- Значения взвешенных сумм $s_j^q$ (до применения функции активации) необходимо сохранять для вычисления производных в процессе обратного распространения.
- Входной слой ($q = 0$) не имеет весов и функций активации — он просто передает входные данные дальше.
