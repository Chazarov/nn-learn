Представлю полный пример одной итерации обучения по алгоритму обратного распространения ошибки.

---

## Дано

**Архитектура сети:** 2 → 3 → 1 (входной, скрытый, выходной слой)

**Функция активации:** ReLU — $f(x) = \max(0, x)$, $f'(x) = \begin{cases} 1, & x > 0 \\ 0, & x \leq 0 \end{cases}$

**Скорость обучения:** $\eta = 0.1$

**Входные данные:** $\mathbf{x} = [0.5,\ 0.3]$

**Желаемый выход:** $d = [1.0]$

**Весовые матрицы** (строка — нейрон-получатель, столбец — нейрон-источник):

$$
W^0 = \begin{pmatrix} 0.1 & 0.4 \\ 0.8 & -0.3 \\ 0.2 & 0.9 \end{pmatrix}, \quad
W^1 = \begin{pmatrix} -0.2 & 0.6 & 0.5 \end{pmatrix}
$$

---

## ШАГ 2. Прямое распространение (forward propagation)

Вычисляем взвешенные суммы и выходы каждого слоя.

### Скрытый слой (q = 0)

Формула: $s_j^0 = \sum_i w_{ij}^0 \cdot x_i$

$$s_0^0 = 0.1 \times 0.5 + 0.4 \times 0.3 = 0.05 + 0.12 = 0.17$$

$$s_1^0 = 0.8 \times 0.5 + (-0.3) \times 0.3 = 0.40 - 0.09 = 0.31$$

$$s_2^0 = 0.2 \times 0.5 + 0.9 \times 0.3 = 0.10 + 0.27 = 0.37$$

Применяем ReLU:

$$y_0^0 = f(0.17) = 0.17, \quad y_1^0 = f(0.31) = 0.31, \quad y_2^0 = f(0.37) = 0.37$$

### Выходной слой (q = 1)

$$s_0^1 = (-0.2) \times 0.17 + 0.6 \times 0.31 + 0.5 \times 0.37 = -0.034 + 0.186 + 0.185 = 0.337$$

$$y_0^1 = f(0.337) = 0.337$$

**Выход сети:** $\hat{y} = 0.337$, желаемое $d = 1.0$

**Ошибка (MSE):** $E = \frac{1}{2}(0.337 - 1.0)^2 = \frac{1}{2} \times 0.4396 \approx 0.2198$

---

## ШАГ 3. Локальные ошибки выходного слоя

Формула (2) из readme:

$$\delta_j^Q = (y_j^Q - d_j) \cdot f'(s_j^Q)$$

$$\delta_0^1 = (0.337 - 1.0) \cdot f'(0.337) = (-0.663) \cdot 1 = -0.663$$

> $f'(0.337) = 1$, так как $0.337 > 0$ (ReLU)

---

## ШАГ 4. Локальные ошибки скрытого слоя

Формула (1) из readme — **векторная форма:**

$$\boldsymbol{\delta}^q = \left[ (W^{q+1})^T \cdot \boldsymbol{\delta}^{q+1} \right] \odot f'(\mathbf{s}^q)$$

### 4.1. Транспонируем матрицу весов следующего слоя

$$W^1 = \begin{pmatrix} -0.2 & 0.6 & 0.5 \end{pmatrix}
\quad \Rightarrow \quad
(W^1)^T = \begin{pmatrix} -0.2 \\ 0.6 \\ 0.5 \end{pmatrix}$$

### 4.2. Матричное умножение $(W^1)^T \cdot \boldsymbol{\delta}^1$

$$(W^1)^T \cdot (-0.663) = \begin{pmatrix} -0.2 \times (-0.663) \\ 0.6 \times (-0.663) \\ 0.5 \times (-0.663) \end{pmatrix} = \begin{pmatrix} 0.1326 \\ -0.3978 \\ -0.3315 \end{pmatrix}$$

### 4.3. Производные активации скрытого слоя

$$f'(\mathbf{s}^0) = \begin{pmatrix} f'(0.17) \\ f'(0.31) \\ f'(0.37) \end{pmatrix} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$$

> Все взвешенные суммы > 0, поэтому производная ReLU = 1

### 4.4. Поэлементное умножение (операция ⊙)

$$\boldsymbol{\delta}^0 = \begin{pmatrix} 0.1326 \\ -0.3978 \\ -0.3315 \end{pmatrix} \odot \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0.1326 \\ -0.3978 \\ -0.3315 \end{pmatrix}$$

---

## ШАГ 5. Вычисление корректировок весов

Формула (3) из readme:

$$\Delta w_{ij}^q = -\eta \cdot \delta_j^q \cdot y_i^{q-1}$$

где $i$ — нейрон-источник (слой $q-1$), $j$ — нейрон-получатель (слой $q$)

### Выходной слой (q = 1), $\mathbf{y}^0 = [0.17,\ 0.31,\ 0.37]$

| Вес | Формула | Результат |
|-----|---------|-----------|
| $\Delta w_{00}^1$ | $-0.1 \times (-0.663) \times 0.17$ | $+0.01127$ |
| $\Delta w_{10}^1$ | $-0.1 \times (-0.663) \times 0.31$ | $+0.02055$ |
| $\Delta w_{20}^1$ | $-0.1 \times (-0.663) \times 0.37$ | $+0.02453$ |

### Скрытый слой (q = 0), $\mathbf{y}^{-1} = \mathbf{x} = [0.5,\ 0.3]$

**Нейрон 0** ($\delta_0^0 = 0.1326$):

| Вес | Формула | Результат |
|-----|---------|-----------|
| $\Delta w_{00}^0$ | $-0.1 \times 0.1326 \times 0.5$ | $-0.00663$ |
| $\Delta w_{10}^0$ | $-0.1 \times 0.1326 \times 0.3$ | $-0.00398$ |

**Нейрон 1** ($\delta_1^0 = -0.3978$):

| Вес | Формула | Результат |
|-----|---------|-----------|
| $\Delta w_{01}^0$ | $-0.1 \times (-0.3978) \times 0.5$ | $+0.01989$ |
| $\Delta w_{11}^0$ | $-0.1 \times (-0.3978) \times 0.3$ | $+0.01193$ |

**Нейрон 2** ($\delta_2^0 = -0.3315$):

| Вес | Формула | Результат |
|-----|---------|-----------|
| $\Delta w_{02}^0$ | $-0.1 \times (-0.3315) \times 0.5$ | $+0.01658$ |
| $\Delta w_{12}^0$ | $-0.1 \times (-0.3315) \times 0.3$ | $+0.00995$ |

---

## ШАГ 6. Корректировка весов

$W^q_{\text{new}} = W^q + \Delta W^q$

### Скрытый слой

$$W^0_{\text{new}} = \begin{pmatrix} 0.1 - 0.00663 & 0.4 - 0.00398 \\ 0.8 + 0.01989 & -0.3 + 0.01193 \\ 0.2 + 0.01658 & 0.9 + 0.00995 \end{pmatrix} = \begin{pmatrix} 0.09337 & 0.39602 \\ 0.81989 & -0.28807 \\ 0.21658 & 0.90995 \end{pmatrix}$$

### Выходной слой

$$W^1_{\text{new}} = \begin{pmatrix} -0.2 + 0.01127 & 0.6 + 0.02055 & 0.5 + 0.02453 \end{pmatrix} = \begin{pmatrix} -0.18873 & 0.62055 & 0.52453 \end{pmatrix}$$

---

## ШАГ 7. Проверка — повторный прямой проход с новыми весами

$$s_0^0 = 0.09337 \times 0.5 + 0.39602 \times 0.3 = 0.04669 + 0.11881 = 0.16549$$

$$s_1^0 = 0.81989 \times 0.5 + (-0.28807) \times 0.3 = 0.40995 - 0.08642 = 0.32353$$

$$s_2^0 = 0.21658 \times 0.5 + 0.90995 \times 0.3 = 0.10829 + 0.27298 = 0.38127$$

$$s_0^1 = (-0.18873) \times 0.16549 + 0.62055 \times 0.32353 + 0.52453 \times 0.38127$$

$$= -0.03123 + 0.20075 + 0.19998 = 0.36950$$

$$\hat{y}_{\text{new}} = 0.3695, \quad E_{\text{new}} = \frac{1}{2}(0.3695 - 1.0)^2 = 0.1989$$

**Ошибка уменьшилась:** $0.2198 \to 0.1989$ — сеть обучается в правильном направлении.
