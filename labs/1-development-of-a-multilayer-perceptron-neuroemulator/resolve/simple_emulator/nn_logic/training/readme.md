## Обратное распространение ошибки (Backpropagation)

```В данном описании явно прописано о скалярной форме записи формулы. Во многих источниках формулы заданы в векторной форме, поэтому важно чтобы складывалось понимание общих формул в ЛЮБОМ их виде.```

Обратное распространение ошибки — это алгоритм обучения нейронной сети с учителем, который вычисляет градиент функции ошибки по весам сети путем распространения ошибки от выходного слоя обратно к входному слою. Алгоритм использует метод градиентного спуска для минимизации ошибки предсказаний сети, итеративно корректируя веса в направлении, противоположном градиенту функции ошибки.

## (MSE) Функция ошибки метода наименьших квадратов

**Скалярная форма**
$$ E(\mathbf{w}) = \frac{1}{2} \sum_{j,k} (y_{j,k}^Q - d_{j,k})^2 $$

*где $\mathbf{w}$ — вектор весов нейронной сети; $y_{j,k}^Q$ — предсказание $Q$-го нейрона для примера $(j,k)$; $d_{j,k}$ — истинное значение для примера $(j,k)$; сумма по всем примерам обучающего множества.*

## Алгоритм обратного распространения по шагам

### (1) Рекурсивная формула локальной ошибки для слоя q



**Скалярная форма**

$$ \delta_j^q = \left[ \sum_r \delta_r^{q+1} w_{jr}^{q+1} \right] \frac{dy_j}{ds_j} $$

*где $\delta_j^q$ — локальная ошибка $j$-го нейрона слоя $q$; $\delta_r^{q+1}$ — ошибка $r$-го нейрона слоя $q+1$; $w_{jr}^{q+1}$ — вес связи от $j$ к $r$ между слоями $q+1$ и $q$; $\frac{dy_j}{ds_j}$ — производная функции активации $j$-го нейрона.*


**Векторная форма**

$$ \beta^q = f'(z^q) \cdot (W^{q+1})^T \cdot \beta^{q+1} $$

*где $\beta^q$ — локальная ошибка слоя $q$; $f'(z^q)$ — производная функции активации слоя $q$; $(W^{q+1})^T$ — транспонированная матрица весов между слоями $q+1$ и $q$; $\beta^{q+1}$ — локальная ошибка слоя $q+1$; **T** - транспонирование: строки исходной матрицы $W^{q+1}$ становятся столбцами (нужно для корректного умножения вектора ошибки $\beta^{q+1}$ на веса при обратном проходе).*


### (2) Формула локальной ошибки выходного слоя

**Скалярная форма**

$$ \delta_j^Q = (y_j^Q - d_j) \frac{dy_j}{ds_j} $$

*где $\delta_j^Q$ — локальная ошибка $j$-го нейрона выходного слоя $Q$; $y_j^Q$ — предсказание $j$-го нейрона выходного слоя; $d_j$ — истинное (желаемое) значение $j$-го выхода; $\frac{dy_j}{ds_j}$ — производная функции активации $j$-го нейрона выходного слоя.*

### (3) Формула обновления весов

**Скалярная форма**

$$ \Delta w_{ij}^q = -\eta \cdot \delta_j^q \cdot y_i^{q-1} $$

*где $\Delta w_{ij}^q$ — изменение веса связи от $i$-го нейрона слоя $q-1$ к $j$-му нейрону слоя $q$; $\eta$ — скорость обучения; $\delta_j^q$ — локальная ошибка $j$-го нейрона слоя $q$; $y_i^{q-1}$ — выход $i$-го нейрона предыдущего слоя $q-1$.*


## (4) Формула обновления весов с моментом

Отличие от (3): добавлен член $\mu \Delta w(t-1)$ — предыдущее направление изменения веса.

Зачем: ускоряет обучение, сглаживает колебания градиента, помогает выбраться из локальных минимумов, предотвращает застревание.

**Формула:**

$$ \Delta w_{ij}^q(t) = -\eta \cdot \left( \mu \cdot \Delta w_{ij}^q(t-1) + (1-\mu) \cdot \delta_j^q \cdot y_i^{q-1} \right) $$

*где $\Delta w_{ij}^q(t)$ — изменение веса на шаге $t$; $\eta$ — скорость обучения; $\mu$ — коэффициент момента (обычно 0.9); $\Delta w_{ij}^q(t-1)$ — изменение веса на предыдущем шаге; $\delta_j^q$ — локальная ошибка; $y_i^{q-1}$ — выход предыдущего слоя.*


**ШАГ 1:** Присвоить всем изменяемым весовым коэффициентам НС небольшими случайными значениями в диапазоне от [-1, 1]

**ШАГ 2:** Подать на входы сети один из возможных образов и в режиме обычного функционирования Нейронной сети, когда сигналы распространяются от входов к выходам, рассчитать значения последних.

**ШАГ 3:** Рассчитать по (2) значение для выходного слоя.

**ШАГ 4:** Рассчитать локальные ошибки  $\beta^q$  для всех скрытых слоев по формуле (1) (от последнего к первому): 
- $\beta^q$ — локальная ошибка (дельта) слоя $q$: $\beta^q = f'(z^q) \cdot (W^{q+1})^T \cdot \beta^{q+1}$.

**ШАГ 5:** Рассчитать для всех слоев по формуле (3) или (4) значения $\delta w^q$.
- $\delta w^q$ — изменение весов (дельта-корректировка) для слоя $q$: $\delta w^q = \alpha \cdot \beta^q \cdot O^{q-1}$ (где $\alpha$ — скорость обучения, $O^{q-1}$ — выход предыдущего слоя).



**ШАГ 6:** Скорректировать все веса в Нейронной сети на рассчитанные изменения.

**ШАГ 7:** Подсчитать ошибку, если ошибка сети существенна, перейти на шаг 2, в противном случае – конец.